{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Gradient Descent\n",
    "- Application: Gradient descent is used to estimate the parameters of a model, in particular coefficients in linear regression or weights in neural networks\n",
    "- Description: Gradient descent is an iterative optimization algorithm for finding the minimum of a (loss) function and the model parameters that minimize that (loss) function\n",
    "- Functionality:\n",
    "    * The cost function is a quadratic and most of the time convex function, meaning it has one global minimum\n",
    "    * First step is to find the gradient, which is the first partial derivative of the cost function ($\\frac{\\delta}{\\delta\\theta_j}J(\\theta)$), and initialize the model parameters $\\theta$ at random\n",
    "    * In order to find the cost function's minimum we move to the negative of the gradient; those moves will be big in the beginning and become smaller as the gradient approaches 0 (the minimum of the cost function)\n",
    "    * We also introduce a learning rate $\\alpha$ that scales the impact of the gradient on $\\theta$; we can have the learning rate gradually decrease towards the end of the iterations to avoid overshooting the minimum\n",
    "    * After each move, we adjust $\\theta$ by the negatige gradient adjusted by the learning rate\n",
    "    * The gradient descent can hence be formalized as: $\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j}J(\\theta)$\n",
    "    * There are different variants of the gradient descent:\n",
    "        * Batch: Compute gradient in one-shot with all training data; disadvantage: very memory expensive especially for large datasets\n",
    "        * Stochastic: Compute gradient using one random training data instance at a time\n",
    "        * Mini-batch: Compute gradient using specified number (mini-batch size) of random training data instances\n",
    "- Limitations: We don't always arrive at the true minimum, however the advantage of this approach is that we don't have to calculate the second derivative or inverse, making it faster than the normal equation of Hessian when the amount of features is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The betas for the model are: [-1.07  1.9  -2.11 -1.15]\n",
      "The interception for the model is: 22.53\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = np.asarray(boston[\"target\"])\n",
    "x = np.asarray(features[[\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]].values)\n",
    "\n",
    "# initiate StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit scaler with training data\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# add a column of 1s for intercept\n",
    "ones_x = np.ones((len(x), 1))\n",
    "x_new = np.append(ones_x, x, axis = 1)\n",
    "\n",
    "# initialize values for gradient descent\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = len(x)\n",
    "theta = np.random.randn(5,1)\n",
    "\n",
    "# run gradient descent\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * x_new.T.dot(x_new.dot(theta) - [[i] for i in y])\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "# print results\n",
    "print(\"The betas for the model are: {}\".format(np.around([i[0] for i in theta][1:], decimals = 2)))\n",
    "print(\"The interception for the model is: {}\".format(np.around([i[0] for i in theta][0], decimals = 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Newton-Raphson\n",
    "- Application: Newton-Rapshon is applied to approximate roots of a function (i.e. Log-Likelihood function)\n",
    "- Description: Newton-Raphson method approximates $f(x)$ with Taylor expansion in the vicinity of the previous iteration\n",
    "- Functionality:\n",
    "    * We choose an intitial value $x_0$ for the algorithm\n",
    "    * We write and adjust the Taylor expansion to be: $x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}$\n",
    "    * We check the stopping condition $|x_{i+1} - x_i| \\leq \\epsilon$, where $\\epsilon$ is a preselected small number\n",
    "    \n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/newtonRaphsonMethod.png\"\n",
    "     width=\"500\" height=\"300\" />   \n",
    "- Limitations: This method requires the derivative of the original function, which can be very complicated. Also the initial guess needs to be fairly accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two roots are -0.81 and 2.47.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define newton raphson optimizer\n",
    "def newton_raphson(start_value, input_function, epsilon):\n",
    "    x0 = start_value\n",
    "    x1 = x0 - (input_function(x0) / ((input_function(x0 + 0.0000001) - input_function(x0)) / (0.0000001)))\n",
    "    while (abs(x1 - x0) > epsilon):\n",
    "        x0 = x1\n",
    "        x1 = x0 - (input_function(x0) / ((input_function(x0 + 0.0000001) - input_function(x0)) / (0.0000001)))\n",
    "    return x1\n",
    "\n",
    "# create test function\n",
    "def test_func(x_input):\n",
    "    return x_input**2 * 3 - x_input*5 - 6\n",
    "\n",
    "# plot test function\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "plot = plt.plot(x, [test_func(i) for i in x])\n",
    "plot = plt.axhline(y=0, color='k')\n",
    "\n",
    "# calculate roots\n",
    "print('The two roots are {:.2f} and {:.2f}.'.format(newton_raphson(-2, test_func, 0.00001), newton_raphson(2, test_func, 0.00001)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Grid Search for Hyperparameter Optimization\n",
    "- Application: Grid Search is used to find the optimal hyperparameters of a model which results in the most accurate predictions\n",
    "- Description:\n",
    "    * Parameters: Internal characteristic of a model, meaning it can be estimated from the data (i.e. beta coefficients for linear regression)\n",
    "    * Hyperparameters: Characteristic of a model that is external, meaning it cannot be estimated from the data. The hyperparameter needs to be determined when fitting the model (i.e. k for KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters are: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=10,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "best parameters are: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features='auto', max_leaf_nodes=40,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "best parameters are: GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "                           max_features=None, max_leaf_nodes=30,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "y = iris['Species']\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "\n",
    "# initialize models\n",
    "models = {'dt': DecisionTreeClassifier(), 'rf': RandomForestClassifier(), 'gb': GradientBoostingClassifier()}\n",
    "\n",
    "# set parameters for each model\n",
    "params = {'dt' : {'criterion':('gini', 'entropy'), 'max_depth':[5,7,9,11,13,15,None], 'max_leaf_nodes':list(range(10,100,5))},\n",
    "          'rf': {'criterion':('gini', 'entropy'), 'max_depth':[5,7,9,11,13,15,None], 'max_leaf_nodes':list(range(10,100,5))},\n",
    "          'gb': {'max_depth':[5,7,9,11,13,15,None], 'max_leaf_nodes':list(range(10,100,5))}}\n",
    "          \n",
    "# run grid search for each model\n",
    "def fit(x_train, y_train):\n",
    "    \"\"\"\n",
    "    fits the list of models to the training data, thereby obtaining in each case an evaluation score after GridSearchCV cross-validation\n",
    "    \"\"\"\n",
    "    for name in models.keys():\n",
    "        est = models[name]\n",
    "        est_params = params[name]\n",
    "        gscv = GridSearchCV(estimator = est, param_grid = est_params, cv = 5)\n",
    "        gscv.fit(x_train, y_train)\n",
    "        print(\"best parameters are: {}\".format(gscv.best_estimator_))\n",
    "\n",
    "# apply function\n",
    "fit(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
