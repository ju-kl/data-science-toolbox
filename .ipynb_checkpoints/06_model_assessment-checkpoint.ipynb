{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Log Loss Function\n",
    "- Description: The Log Loss Function is used to measure the performance of a classification model where the prediction is a probability between 0 and 1.\n",
    "- Functionality:\n",
    "    * We penalize wrong predictions by adding to the loss function and do not increase the loss function if a correct prediction was made by the classification model.\n",
    "    * Formula: $logloss = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{i=1}^{M} y_{ij} ln(p_{ij})$ where $N$ is the number of objects to classify, $M$ is the number of classes, $y_{ij}$ is 1 if the object $i$ belongs the the class $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that object $i$ belongs to class $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Accuracy, Precision, Recall, F1, R Squared Scores\n",
    "- Confusion Matrix:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1594/1*CPnO_bcdbE8FXTejQiV2dg.png\"\n",
    "     width=\"500\" height=\"300\" />\n",
    "     \n",
    "- Accuracy:\n",
    "    * Fraction of predictions our model got right\n",
    "    * Good measure to report to senior leadership as its an intuitive way of measuring performance\n",
    "    * $Accuracy = \\frac{n~correct~predictions}{n~total~preductions}$\n",
    "- Precision:\n",
    "    * How many of the actual positive cases did we find\n",
    "    * Good measure to determine model accuracy, when the cost of False Negative is high (i.e. fraud detection)\n",
    "    * $Precision = \\frac{True~Positive}{True~Positive + False~Negative} = \\frac{True~Positive}{Total~Actual~Positive}$\n",
    "- Recall:\n",
    "    * Of all our predicted positive cases, how many of them are actual positive cases\n",
    "    * Good measure to determine model accuracy, when the costs of False Positive is high (i.e. email spam detection)\n",
    "    * $Recall = \\frac{True~Positive}{True~Positive + False~Positive} = \\frac{True~Positive}{Total~Predicted~Positive}$\n",
    "- F1: \n",
    "    * Weighed average of precision and recall\n",
    "    * Good measure when you want to seek a balance between Precision and Recall\n",
    "    * $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "- R Squared:\n",
    "    * Provides a measure of how well observed outcomes are replicated by a model\n",
    "    * Measures the proportion of the variance in the target variable that is predictable from the model's features\n",
    "    * Adjusted R Squared takes into consideration the number of predictors, accounting for overfitting\n",
    "    * $ R^2 = 1 - \\frac{SSE}{SSE + SSR} = 1 - \\frac{SSR}{SST}$\n",
    "        * SSE (sum of squares of residuals): Measures the unexplained variance of the model, hence the standard deviation of the error $\\sum{(y_i - \\hat{y})}$\n",
    "        * SSR (sum of squares of regression): Meaures the explained variance of the model $\\sum{(\\hat{y_i} - \\bar{y})}$\n",
    "        * SST (total sum of squares): Measures the total variance of the data $\\sum{(y_i - \\bar{y})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 ROC and AUC\n",
    "- ROC:\n",
    "    * Plots all possible decision thresholds with the corresponding true positive/false positive rates and connects those to a curve\n",
    "    * Y-axis: True Positive Rate ($precision = \\frac{True~Positive}{True~Positive + False~Negative}$); x-axis: False Positive Rate ($\\frac{False~Positive}{False~Positive + True~Negative}$)\n",
    "    * Hence the ROC curve summarizes all possible confusion matrices at each threshold produced\n",
    "    * Helps to identify the best threshold (on the top left corner)\n",
    "- AUC:\n",
    "    * Area under the ROC graph; the higher the better\n",
    "    * Used to compare different ROC graphs to each other\n",
    "    \n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/ROC-Curve-Plot-for-a-No-Skill-Classifier-and-a-Logistic-Regression-Model.png\"\n",
    "     width=\"500\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Root Mean Square Error\n",
    "- Description: The Root Mean Square Error (RMSE) is used to measure the performance of regression models\n",
    "- Functionality: \n",
    "    * Measured the differences between predicted values by a model and the actual values\n",
    "    * The RMSE will have the the same unit as the dependent variable, meaning that there is no absolute good or bad value    \n",
    "    * The formula to find the RMSE is: $RMSE = \\sqrt{\\sum_{i=1}^{N} \\frac{(\\hat{y_i} - y_i)^2}{N}}$\n",
    "    * There are different variations of this formula (i.e. Mean Squared Error)\n",
    "    * MSE consists of the sum of: the variance of the function, its squared bias and the irreducible error ($\\sigma^2$)\n",
    "- Application:\n",
    "    * Training set: Use training dataset to train model, and apply model to test dataset to predict; then measure error between prediction and actual\n",
    "    * Test set: Use train dataset to train model and apply the model back to the train data for predictions; then measure error between prediction and actual\n",
    "    * Cross-Validation: See below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Cross Validation\n",
    "- Description: Cross validation is a resampling procedure used 1) to determine how well a model can be expected to perform on new data and 2) to determine the best model hyperparameters by finding the minimum training error\n",
    "- Functionality:\n",
    "    * We first split all data into a training and test set \n",
    "    * We then divide the training data into k-folds, with k being some arbitrary number\n",
    "    * The goal is to train the model on k-1 folds and and validate it on the one remaining fold\n",
    "    * Assuming we settle on 5 folds, we will train the model on 4 folds and assess its proformance on the 5th fold\n",
    "    * This process is repeated 4 more times, iterating over the remaining 4 training and 1 test folds\n",
    "    * Once 5 models are fit, we combine the validation results be i.e. averaging the results of the 5  rounds to give an estimate of the model's predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Overfitting vs. Underfitting \n",
    "- Variance refers to the amount by which the model would change if we introduced it to new training data (how closely does our model model the data points) vs. Bias refers to the error by approximating a real-world problem with a much simpler model\n",
    "- In general, the more flexible/complicated a model is, the higher its variance and the lower its bias\n",
    "- Overfitting: \n",
    "    * Modeling error occuring when a model is too closely fit to the training set data points and hence captures its noise due to its high complexity\n",
    "    * The model suffers from high variance, meaning that the model will be inconsistent over different training/testing sets\n",
    "    * Identification: $training~error < test~error$ OR $CV~error > training~error$\n",
    "    * Counter-measure: Decrease the model complexity, increase the minimum samples per leaf\n",
    "- Underfitting:\n",
    "    * Modeling error occuring when a model cannot adequately capture the training data and hence does not fit the data well enough\n",
    "    * The model suffers from high bias, meaning that the model will not be predicting with high accuracy\n",
    "    * Identification: high $training~error$ in general OR $CV~error = training~error > test~error$\n",
    "    * Counter-measure: Increase the model complexity, decrease the minimum samples per leaf\n",
    "- Variance-Bias Tradeoff:\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/GEJIM.png\" width=\"500\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Misc Notes\n",
    "- Problem of **high-dimensionality data** ($p >> n$): We can perfectly fit our data without any error\n",
    "- **Training MSE** usually underestimates the test MSE, because we fit the model to the training data, minimizing the error; models with many parameter will explain more variance in the training data, but might be unstable on the test data, hence we measure their performance including the price paid for additional predictors in order to eliminate noisy predictors (i.e. AIC, adjusted $R^2$, BIC)\n",
    "- **Bayes decision boundary**: Classifies an observation to the class for which $p(x)$ is largest; for 2 points the boundary is where the probability is equal to 50%; in general Bayes classifier produces the lowest possible error rate\n",
    "- **Regularization** (technique to deal with overfitting):\n",
    "    * **Subset Selection**:\n",
    "        * Goal: Reduce the number of features in the model only selecting the most predictive ones\n",
    "        * Fit $p^2$ separate least squared linear regression models for each possible combination of the $p$ predictors and then select the best model\n",
    "        * Selection is based on i.e. CV error, test set error or adjusted $R^2$ to account for the increase in predictors\n",
    "        * This approach is computationally intensive esp. for large $p$\n",
    "        * Alternatives are:\n",
    "            * **Forward Selection**: Calculate null model, then calculate all models that contain one additional predictor, choose models with smallest RSS, select best model using cross validation/AIC/BIC\n",
    "            * **Backward Selection**: Calculate full model, then calculate all models that contain all but one predictor, choose models with smallest RSS, select best model using cross validation/AIC/BIC\n",
    "            * The problem with this is that we commit to a fixed path which might not result in the selection of the optimal features\n",
    "    * **Shrinkage**:\n",
    "        * See chapters on lasso and ridge regression models (applied to fit a linear regression given a large number of predictors with questionable effect on the model's accuracy)\n",
    "    * **Dimension Reduction**:\n",
    "        * Above approach is focused on selecting the right predictors, this approach is focused on transforming the predictors; for more details see section on dimension reduction\n",
    "- Methods to make linear models non-linear:\n",
    "    * **Polynomial regression**: Rasining each of the predictor to a power; i.e. cubic regression uses three variables $X, X^2, X^3$\n",
    "    * **Step functions**: Cut range of a predictor into $K$ distinct regions, producing a qualitative variable to then fit a piecewise constant function\n",
    "    * **Regression splines**: Divide range of predictor into $K$ distinct regions, fit polynomial function within each region, functions are constrained so that they join smoothly at the region boundaries/knots\n",
    "    * **Smoothing splines**: Result from minimizing least squares subject only to a smoothness penalty (second derivative of function/roughness)\n",
    "    * **Local regression**: Divide data into regions based on their neighbors in which unique regressions are built and merged in a smooth way\n",
    "    * **Generalized additive models**: Add different models of multiple predictors; be aware of missing interaction if not specified separately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
