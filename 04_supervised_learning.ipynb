{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Supervised Learning\n",
    "This section presents several supervised learning algorithms in the following way:\n",
    "- Application: When should this algorithm be used?\n",
    "- Description: What does this algorithm do (high-level)?\n",
    "- Functionality: How does this algorithm work (drill-down)?\n",
    "- Limitations: What does one have to keep in mind when using this algorithm?\n",
    "- Example: How is this algorithm applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Linear Regression\n",
    "- Application: Linear Regression is used to quanify the strength of the linear relationship between a target and explanatory variables and based on this relationship predict the target variable\n",
    "- Description: Linear Regression is a linear approach to model the relationship between a scalar target variable and one or more explanatory variables applying i.e. least squares minimization\n",
    "- Functionality:\n",
    "    * We can think of a Linear Regression as an orthogonal projection of the target variable $(\\vec{y})$ onto the $span($$\\vec{1}$, $\\vec{x}$$)$\n",
    "    * The equation for a Linear Regression is: $y$ = $\\beta_{\\,0}$ + $\\beta_{\\,1}$$x_{\\,1}$ + ... + $\\beta_{\\,n}$$x_{\\,n}$ + $\\epsilon$\n",
    "    * Our goal is to minimize the following adjusted form of the above equation (least squares minimization): $||$ $\\vec{y}$ - ($\\vec{\\beta}_{\\,0}$*$\\vec{1}$ + $\\vec{\\beta}_{\\,1}$*$\\vec{x}_{\\,1}$ + ... + $\\vec{\\beta}_{\\,n}$$\\vec{x}_{\\,n}$) $||^{\\,2}$; representing the minimization of the sum of squared residuals\n",
    "    * In order to minimize this equation, we optimize $\\vec{\\beta}$ to find a linear combination of $\\vec{1}$ and $\\vec{x}$ that is as close to our target variable $\\vec{y}$ as possible\n",
    "    * We achieve this by projecting $\\vec{y}$ onto the $span($$\\vec{1}$, $\\vec{x}$$)$, representing our predicted values\n",
    "    * The residuals of the model are the differences between the actual values and the predicted values\n",
    "    * $\\vec{\\beta}$ are unbiased parameters, meaning that if we were to sample a huge number of data points we would estimate the parameters of the true regression line\n",
    "    * To determine whether there is a relationship between $y$ and $x$, we test the H0 that the parameters are equal to $0$ (one predictor: reject for p-value < 0.05, multiple predictors: reject for F-statistic > 1)\n",
    "    * Interaction effect: Changing the value of one predictor effects the values of the other predictors; if this is the case, we have to include an interaction term in the model\n",
    "    * Polynomial regression: Transform the predictors i.e. with polynomials to better achieve a better accuracy\n",
    "    * Key assumptions: Linearity between $y$ and $x$, uncorrelated error terms, no pattern in residuals (normal distribution), homoscedasticity (constant variance of residuals), no (multi-)collinearity (independence of predictors, can be detected with Variance Inflation Factor)\n",
    "- Limitations: It is very sensitive to outliers\n",
    "- Note: If $y$ is not normally distributed, we need to fit Generalized Linear Models (i.e. $y$ ~ $Binom$, $y$ ~ $Pois$, $y$ ~ $Gamma$), which are estimated with the Maximum Likelihood function and have different links connecting the linear predictor and the expectation of the output (i.e. logit, probit, complementary log-log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The betas for the model are: [-0.13  0.08 -0.25 -0.04]\n",
      "The interception for the model is: 26.49\n",
      "Prediction: 22.75 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features[[\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]].values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# fit regression model\n",
    "reg = LinearRegression().fit(x, y)\n",
    "\n",
    "# predict\n",
    "prediction = reg.predict(x_holdout) \n",
    "\n",
    "# print results\n",
    "print(\"The betas for the model are: {}\".format(np.around(reg.coef_, decimals = 2)))\n",
    "print(\"The interception for the model is: {}\".format(np.around(reg.intercept_, decimals = 2)))\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(prediction[0], y_holdout[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Ridge and Lasso Regression\n",
    "- Application: Ridge and Lasso Regression are applied to fit a linear regression given a large number of predictors with questionable effect on the model's accuracy (feature selection), especially if $p > n$, where $n$ - number of observations, $p$ - number of predictors; they can also be used to balance the parameters of the model\n",
    "- Description: Ridge and Lasso Regression are techniques that regularize the coefficients of the regression, or respectively shrink them to zero if necessary in order to reduce varaince of the model by penalizeing cost functions by introducing more bias\n",
    "- Functionality:\n",
    "    * Ridge:\n",
    "        * Ridge coefficients minimize: $\\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2$\n",
    "        * The tuning parameter $\\lambda$ shrinks every dimension of the data by the same proportion\n",
    "        * The final model will still have all predictors, but the effect of predictors with small coefficients will be reduced\n",
    "        * As the tuning parameter $\\lambda$ increases, the flexibility of the model fit decreases, leading to reduced variance and higher bias\n",
    "        * Ridge regression works best if the linear regression has high variance and if there is multicollinearity\n",
    "    * Lasso:\n",
    "        * Lasso coefficients minimize: $\\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j|$\n",
    "        * The tuning parameter $\\lambda$ shrinks all coefficients toward zero by the same amount \n",
    "        * The adjusted form of the shrinkage penalty has the effect of forcing some of the coefficients to be exactly equal to exactly zero when the tuning parameter $\\lambda$ is sufficiently large; this is because of the quadratic shape of the lasso equation compared to the circular shape of the ridge regression for the error contour\n",
    "        * Hence, Lasso actually performs feature selection\n",
    "    * Since the shrinkage penelty does not only impact the $\\beta$ but also the value predictor, all predictors should be standardized before fitting a Ridge or Lasso regression\n",
    "    * The best value for $\\lambda$ is found with cross validation\n",
    "    * Lasso and Ridge are generally less flexible than linear regression\n",
    "- Limitations: Lasso produces simpler results and more interpretable models that only involve a subset of the predictors than Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The betas for the model are: [-1.100e-01  5.000e-02  1.000e-02  2.630e+00 -1.545e+01  3.790e+00\n",
      " -0.000e+00 -1.460e+00  3.000e-01 -1.000e-02 -9.000e-01  1.000e-02\n",
      " -5.300e-01]\n",
      "The interception for the model is: 34.81\n",
      "Prediction: 26.43 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features.values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# initialize and fit model\n",
    "ridge = linear_model.Ridge(alpha=0.2).fit(x, y)\n",
    "\n",
    "# predict\n",
    "prediction = ridge.predict(x_holdout) \n",
    "\n",
    "# print results\n",
    "print(\"The betas for the model are: {}\".format(np.around(ridge.coef_, decimals = 2)))\n",
    "print(\"The interception for the model is: {}\".format(np.around(ridge.intercept_, decimals = 2)))\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(prediction[0], y_holdout[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The betas for the model are: [-0.09  0.05 -0.02  0.   -0.    3.37 -0.   -1.13  0.28 -0.02 -0.75  0.01\n",
      " -0.6 ]\n",
      "The interception for the model is: 27.46\n",
      "Prediction: 27.39 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features.values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# initialize and fit model\n",
    "lasso = linear_model.Lasso(alpha=0.2).fit(x, y)\n",
    "\n",
    "# predict\n",
    "prediction = lasso.predict(x_holdout) \n",
    "\n",
    "# print results\n",
    "print(\"The betas for the model are: {}\".format(np.around(lasso.coef_, decimals = 2)))\n",
    "print(\"The interception for the model is: {}\".format(np.around(lasso.intercept_, decimals = 2)))\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(prediction[0], y_holdout[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic Regression\n",
    "- Application: Logistic Regression is used to model the probabilities for a binary target variable/event/class\n",
    "- Description: Logistic Regression is a non-linear classification model that is expressed as a linear model. It models probabilities for a binary target variable based on many more explanatory variables from different domains (categorical and/or continuous variable)\n",
    "- Functionality:\n",
    "    * If we'd use a linear regression to model this relationship it would look like this: $p(x) = \\beta_0 + \\beta_1x_1$; the problem with this equation is that the predicted probabilities don't fall between 0 and 1\n",
    "    * In order to avoid this problem, we can apply different transformations such as the logistic function: $p(x) = {\\frac{exp(\\beta_0 + \\beta_1x_1)}{1 + exp(\\beta_0 + \\beta_1x_1)}}$\n",
    "    * After manipulating this new function we find that: $(\\frac{p}{1-p}) = exp(\\beta_0 + \\beta_1x_1)$\n",
    "    * $(\\frac{p}{1-p})$ is called the odds of the odds of the event $Y=1$ to happen\n",
    "    * If we want to bound this value between 0 and 1, we can again apply different transformations such as the logistic function: $ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_1x_1$\n",
    "    * This is called $log$ $odds$ or $logit$ and shows that the logistic regression has a logit that is linear in $x$: We consider a linear model with a target and multiple explanatory variables and assume a linear relationship between the explanatory variables and the $log(odds)$ of the event $Y=1$: \n",
    "    * In order to transform the  $log$ $odds$ to the function for the probabilities, we have to apply the Sigmoid Function (inverse of the logit function)\n",
    "    * Increaseing $x$ by one unit changes the $log$ $odds$ by $\\beta_1$, or equivalently it multiplies the $odds$ by $exp(\\beta_1)$\n",
    "    * Logistic Regression fits a linear decision boundary; regression coefficients are estimated using the Maximum Likelihood Method\n",
    "- Limitations: Logistic Regression can only predict a categorical outcome and cannot solve non-linear problems because it's decision surface is linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The betas for the model are: [[-0.41021713 -1.46416217  2.26003266  1.02103509]]\n",
      "The interception for the model is: [-0.26421853]\n",
      "The probability for data point with feature values: 5.1, 3.5, 1.4, 0.2 to belong to category 0 is 0.98\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "\n",
    "# make binary classification\n",
    "iris[\"Classification\"] = np.where(iris[\"Species\"] == 0, 0, 1)\n",
    "\n",
    "# create arrays\n",
    "y = iris['Classification']\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "\n",
    "# initialize model\n",
    "reg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "reg.fit(x, y)\n",
    "\n",
    "# predict probabilities\n",
    "proba = reg.predict_proba([[5.1,3.5,1.4,.2]])\n",
    "\n",
    "# print results\n",
    "print(\"The betas for the model are: {}\".format(reg.coef_))\n",
    "print(\"The interception for the model is: {}\".format(reg.intercept_))\n",
    "print(\"The probability for data point with feature values: 5.1, 3.5, 1.4, 0.2 to belong to category 0 is {:.2f}\".format(proba[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 K-Nearest Neighbors\n",
    "- Application: K-Nearest Neighbors (KNN) is used for classification or regression\n",
    "- Description: KNN identifies the k nearest points in the data and estimates the target based on the labels of this set of k nearest points\n",
    "- Functionality:\n",
    "    * First, we have to define k, the number of neighbors we are looking for; this number is usually small and depends on the bias-variance-tradeoff (small k high low bias and high variance and vice versa)\n",
    "    * There are different methods to calculate the distance to the k nearest neighbors (i.e. euclidean, manhattan), selecting the right method is important\n",
    "    * For classification: The target is classified by a plurality vote of the k nearest neighbors. The class is determined by the most common class among the k nearest neighbors. If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor\n",
    "    * For regression: The target is estimated simply by the average of the values of the k nearest neighbors\n",
    "    * It is common to assign weights to the contributions of the neighbors in order for nearer neighbors to contribute more to the target than the more distant ones\n",
    "- Limitations: KNN is a lazy learner, meaning that it does not learn from the training data but rather uses it for its prediction. This is referred to as memory-based approach, because  we need all training data at each time we want to make a prediction; this makes KNN rather slow, because it has to compute the distance to all points in the training set, sort them, and make a prediction; KNN is not robust to noisy data; KNN does not work well for high dimensionality data because of the curse of dimensionality (there are no points that are close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 8.77; Train set RMSE is 7.13\n",
      "Prediction: 23.18 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features[[\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]].values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize knn\n",
    "knn = KNeighborsRegressor(n_neighbors = 10)\n",
    "\n",
    "# fit knn\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = knn.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = knn.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# train rmse\n",
    "train_rmse = MSE(y_train, y_pred_train)**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'Train set RMSE is {:.2f}'.format(train_rmse))\n",
    "\n",
    "# predict\n",
    "prediction = knn.predict(x_holdout)\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 0.18; Train set RMSE is 0.18\n",
      "Prediction: 2.00 vs. Actual: 2.00\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "y = iris['Species']\n",
    "y = y[:149]\n",
    "y_holdout = y[148:]\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "x = x[:149]\n",
    "x_holdout = x[148:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize knn\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "\n",
    "# fit knn\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = knn.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = knn.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# train rmse\n",
    "train_rmse = MSE(y_train, y_pred_train)**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'Train set RMSE is {:.2f}'.format(train_rmse))\n",
    "\n",
    "# predict\n",
    "prediction = knn.predict(x_holdout)\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Apriori Algorithm\n",
    "- Application: Apriori is used to find relationships/associations of items in data sets (i.e. market basked analysis)\n",
    "- Description: Apriori determines item sets that appear sufficiently often together in the data. These item sets are used to define association rules which highlight general trends in the data\n",
    "- Functionality:\n",
    "    * The algorithm is based on the following three constructs: Support, Confidence, and Lift. When running the algorithm, we have to define minimum values for each of the three consturcts\n",
    "    * Support: Represents the popularity of an item $Support(B) = \\frac{trx_B}{trx_{all}}$\n",
    "    * Confidence: Represents the likelihood that item $B$ is also purchased when item $A$ is purchased: $Confidence(A→B) = \\frac{trx_{AB}}{trx_A} $\n",
    "    * Lift: Represents the increase in the ratio of sale B when A is sold: $Lift(A→B) = \\frac{Confidence(A→B)}{Support(B)} $\n",
    "    * Marketing campagins are focused on the lift specifically. $Lift = 1$ menas there is no association between the products. $Lift > 1$ means that item A & B are more likely to be bought together.\n",
    "    * This algorithm works solely based on the purchase history of items and we do not need any features to the items to run it\n",
    "    * A strong pattern would be an item with low support, meaning it is not purchased very often, but high confidence, meaning that if its purchased it is purchased in combination with other itmes.\n",
    "- Limitations: Apriori might be slow as it calculates all possible combinations between items, becomeing a problem if there is a large number of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support      itemsets  length\n",
      "0    0.625       (Apple)       1\n",
      "1    0.750        (Beer)       1\n",
      "2    0.500        (Rice)       1\n",
      "3    0.500  (Beer, Rice)       2\n",
      "  antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0      (Beer)      (Rice)                0.75                0.50      0.5   \n",
      "1      (Rice)      (Beer)                0.50                0.75      0.5   \n",
      "\n",
      "   confidence      lift  leverage  conviction  \n",
      "0    0.666667  1.333333     0.125         1.5  \n",
      "1    1.000000  1.333333     0.125         inf  \n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "dataset = [['Apple', 'Beer', 'Rice', 'Chicken'],\n",
    "           ['Apple', 'Beer', 'Rice'],\n",
    "           ['Apple', 'Beer'],\n",
    "           ['Apple', 'Bananas'],\n",
    "           ['Milk', 'Beer', 'Rice', 'Chicken'],\n",
    "           ['Milk', 'Beer', 'Rice'],\n",
    "           ['Milk', 'Beer'],\n",
    "           ['Apple', 'Bananas']]\n",
    "\n",
    "# encode transactions\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(dataset).transform(dataset).astype('int')\n",
    "df = pd.DataFrame(te_array, columns = te.columns_)\n",
    "\n",
    "# apply apriori\n",
    "frequent_itemsets = apriori(df, min_support = 0.4, use_colnames = True)\n",
    "\n",
    "# length of sets\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# find association rules\n",
    "rules = association_rules(frequent_itemsets, metric = 'lift', min_threshold = 1)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Support Vector Machines\n",
    "- Application: Support Vector Machienes (SVM) are generally used for classification problems\n",
    "- Description: SVMs fit a linear decision boundary that best divides a dataset into two or more classes. SVM can also solve non-linear classification problems, applying the Kernel Trick taht transforms data into higher dimensions in which a linear decision boundary can be fit\n",
    "- Functionality:\n",
    "    * Hyperplane: Linear decision boundary between two classes with $p-1$ dimensions; formula: $\\beta_0 + \\beta_1x_1 + ... + \\beta_px_p = 0$\n",
    "    * Margin: Minimal distance between the hyperplane and the nearest data point from either class\n",
    "    * Support Vectors: Data points closest to the hyperplane; removing support vectors alters the position of the hyperplane whereas data points within classes don't\n",
    "    * **Maximal margin classifier**:\n",
    "        * Goal: Chose a hyperplane with the highest possible margin within the training set, giving a greater chance of new data being classified correctly\n",
    "        * To do so we calculate the perpendicular distance form the training data to the hyperplane; this is only possible if the two classes are perfectly seperable\n",
    "    * **Support vector classifier**: \n",
    "        * Goal: Misclassify a few training observations in order to do a better job in classifying the remaining observations\n",
    "        * To do so we allow a soft margin, meaning that we introduce a budget $C$ for the amount that we can misclassify observations (are around the hyperplane in which data can be misclassified); if $C$ is small, there will be fewer support vectors and hence the classifier will have high variance but low bias\n",
    "    * **Support vector machines** \n",
    "        * Goal: If the decision boundary is non-linear, we transform the data to enlarge the feature space using $kernels$ (an computationally efficient approach to enlarging the feature space)\n",
    "        * As a result we map our input space into as many higher-dimensionality feature spaces as we need in which we can fit a linear decision boundary with support vector classifiers\n",
    "        * Kernel SVMs find the function that best transform the input data into a new feature space in order to separate the classes and the output of that function becomes the new dimension (i.e. polynomial, radial)\n",
    "        * Based on this fitted decision boundary, incoming data is mapped into that same space as the training data and predicted to belong to a category based on the side of the hyperplane on which they fall\n",
    "- Limitations: Don't perform well on multi-class classification, need to transform into one-vs-many classification; less effective on noisier datasets with overlapping classes and high training times on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_svc:               precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        12\n",
      "  Versicolor       1.00      1.00      1.00        11\n",
      "   Virginica       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "y_pred_rbf:               precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        12\n",
      "  Versicolor       1.00      0.91      0.95        11\n",
      "   Virginica       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "y_pred_poly_svc:               precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        12\n",
      "  Versicolor       1.00      0.91      0.95        11\n",
      "   Virginica       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "y_pred_lin_svc:               precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        12\n",
      "  Versicolor       1.00      0.91      0.95        11\n",
      "   Virginica       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "y = iris['Species']\n",
    "y = y[:149]\n",
    "y_holdout = y[148:]\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "x = x[:149]\n",
    "x_holdout = x[148:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize and fit models with different kernels\n",
    "C = 1\n",
    "svc = svm.SVC(kernel = 'linear', C = C).fit(x_train, y_train)\n",
    "rbf = svm.SVC(kernel = 'rbf', gamma = 0.7, C = C).fit(x_train, y_train)\n",
    "poly_svc = svm.SVC(kernel = 'poly', degree = 3, C = C).fit(x_train, y_train)\n",
    "lin_svc = svm.LinearSVC(C = C).fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_svc = svc.predict(x_test)\n",
    "y_pred_rbf = rbf.predict(x_test)\n",
    "y_pred_poly_svc = poly_svc.predict(x_test)\n",
    "y_pred_lin_svc = lin_svc.predict(x_test)\n",
    "\n",
    "# print classification reports\n",
    "print('y_pred_svc:', classification_report(y_test, y_pred_svc, target_names = ['Setosa', 'Versicolor', 'Virginica']))\n",
    "print('y_pred_rbf:', classification_report(y_test, y_pred_rbf, target_names = ['Setosa', 'Versicolor', 'Virginica']))\n",
    "print('y_pred_poly_svc:', classification_report(y_test, y_pred_poly_svc, target_names = ['Setosa', 'Versicolor', 'Virginica']))\n",
    "print('y_pred_lin_svc:', classification_report(y_test, y_pred_lin_svc, target_names = ['Setosa', 'Versicolor', 'Virginica']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Classification And Regression Trees\n",
    "- Application: Classification And Regression Trees (CART) are used for classification and regression problems\n",
    "- Description: Decision Trees learn decision rules from observations about items. To make conclusions about new items' target variable, we apply the decision rules to the values of new items, meaning that we follow the branches of the Decision Tree, compare values and jump to the next node until arriving at a conclusion\n",
    "- Functionality:\n",
    "    * Root node: Doesn't have a parent node, question gets divided into two child nodes\n",
    "    * Internal node: Has a parent node, question gets divided into two child nodes\n",
    "    * Lead node: Has a parent node, but no child nodes; represents the prediction\n",
    "    * Depthness of a decision tree: Number of nodes of the tree\n",
    "    * Classification: **information gain**\n",
    "        * Constructing a classification decision tree is about splitting into child nodes in a way that increases the information gain\n",
    "        * Entropy: Measure of randomness in an event (events with high randomness: hard to draw conclusions from; pure events: expect same outcome for every occurance of the event); formula: $E(S) = \\sum_{i=1}^{N} - p_i log_2 p_i$\n",
    "        * GINI: Measure of total variance across classes in a node; formula: $GINI = 1 - \\sum_{i=1}^{N} p_i^2$\n",
    "        * Information Gain: Decrease in entropy; forumla $IG = Entropy(before$ $split)$ $- Entropy($$after$ $split)$\n",
    "        * Approach: \n",
    "            * Define decision rule based on the items' values that creates the best split of items based on the target variable\n",
    "            * Calculate information gain for the split; if there is no increase in information gain we formulate another decision rule\n",
    "            * Once a decision rule with sufficient information gain is defined, we split into two child notes     \n",
    "            * If we the child nodes have pure events we don't need to split into more nodes; if not we split into more nodes to further increase purity\n",
    "            * Splitting stops when no further information gain can be made or pre-set stopping rules are met\n",
    "            * Each branch ends in a lead node, making each observation fall into exaclty one lead node and each lead node uniquely defined by a set of rules\n",
    "            * The lead nodes' predictions are the mean or mode of all items falling into that leaf node\n",
    "    * Regression: **recursive binary splitting**\n",
    "        * Recursive binary splitting: Greedy approach because at each step of the tree-building process, the best split is made at that particular step ignoring a future split that will lead to an overall better tree\n",
    "        * Approach:\n",
    "            * Define decision rule based on the itmes' values that leads to the highest possible reduction in residual sum of squares (SSR) of the tree\n",
    "            * Repeat this process, looking for the best predictor and the best cutpoints to futher minimize the SSR within each resulting node\n",
    "            * Splitting stops when no further reduction in SSR is possible or pre-set stopping rules are met\n",
    "            * Cost complexity pruning: Obtain a sequence of subtrees indexed at $\\alpha$ (increases with the number of lead nodes), run cross validation on all subtrees and select subtree sith lowest $CV error$; this approach controls the trade-off between the complexity and its fit to the training data\n",
    "- Limitations: Instability due to high variance in sampled data, high sensitivity to the data that its trained to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 8.29; the CV RMSE is 7.90\n",
      "CRIM    0.682594\n",
      "ZN      0.317406\n",
      "AGE     0.000000\n",
      "RAD     0.000000\n",
      "dtype: float64\n",
      "Prediction: 22.36 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features[[\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]].values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize decision tree\n",
    "dt = DecisionTreeRegressor(max_depth = 2, min_samples_leaf = 5, random_state = 3)\n",
    "\n",
    "# fit decision tree\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = dt.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = dt.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# mean cv (runs on train set)\n",
    "mse_cv_scores = - cross_val_score(dt, x_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# compute the 10-folds CV rmse\n",
    "cv_rmse = (mse_cv_scores.mean())**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'the CV RMSE is {:.2f}'.format(cv_rmse))\n",
    "\n",
    "# find feature importance\n",
    "print(pd.Series(dt.feature_importances_, index = [\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]).sort_values(ascending = False))\n",
    "\n",
    "# predict\n",
    "prediction = dt.predict(x_holdout)\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 0.18; the CV RMSE is 0.22\n",
      "PetalWidthCm     0.884093\n",
      "PetalLengthCm    0.089412\n",
      "SepalWidthCm     0.018925\n",
      "SepalLengthCm    0.007570\n",
      "dtype: float64\n",
      "Prediction: 2.0 vs. Actual: 2.0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "y = iris['Species']\n",
    "y = y[:149]\n",
    "y_holdout = y[148:]\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "x = x[:149]\n",
    "x_holdout = x[148:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize decision tree\n",
    "dt = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "# fit decision tree\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = dt.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = dt.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# mean cv (runs on train set)\n",
    "mse_cv_scores = - cross_val_score(dt, x_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# compute the 10-folds CV rmse\n",
    "cv_rmse = (mse_cv_scores.mean())**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'the CV RMSE is {:.2f}'.format(cv_rmse))\n",
    "\n",
    "# find feature importance\n",
    "print(pd.Series(dt.feature_importances_, index = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]).sort_values(ascending = False))\n",
    "\n",
    "# predict\n",
    "prediction = dt.predict(x_holdout)\n",
    "print(\"Prediction: {} vs. Actual: {}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Random Forest\n",
    "- Application: Random Forests are used for classifiaction and regression problems\n",
    "- Description: Random Forests are an ensemble learning method, training a multitude of decision trees on the same dataset and aggregating the predictions of the individual trees (i.e. mode of classes for classification, mean of predictions for regression)\n",
    "- Functionality:\n",
    "    * Esemble Learning: Fit a multitude of independent models on sampeled data from the same dataset and aggregate predictions by majority voting for classification and by averaging for regression\n",
    "    * Base model: Large number of individual and uncorrelated decision trees; all models must be of the same kind\n",
    "    * Bagging/Bootsrapping:\n",
    "        * We take a random sample of observations from the dataset with replacement: If we have a training set with N observations, we still feed each tree a training set of size N, but instead of the original training data, we randomly select N observations with replacement from the training data in order to to produce a bootstrap data set\n",
    "        * Rationale: This process allows us to emulate the process of obtaining new sample sets, so we can estimate the variablility without generating new samples in order to minimize the variance of the predictions\n",
    "    * Feature bagging: \n",
    "        * When splitting into a node, each decision tree in a random forest can only pick from a random subset of features without replacement, forcing more variation amongst the trees in the model and ultimately resulting in lower correlation across trees and more diversification\n",
    "        * Rationale: If one or a few features are very strong predictors for the target variable, these features will be selected in many decision trees; we decorrelate the trees\n",
    "    * We repeat this process multiple times based on the same training data to build different models that are then aggregated repsectively; each individual model has high variance, but averaging the models reduced variance\n",
    "- Limitations: Some features might get sampled often whereas others not so often; difficult to interpret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 7.86; the CV RMSE is 7.90\n",
      "CRIM    0.479596\n",
      "ZN      0.220512\n",
      "AGE     0.216424\n",
      "RAD     0.083469\n",
      "dtype: float64\n",
      "Prediction: 21.17 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features[[\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]].values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize decision tree\n",
    "rf = RandomForestRegressor(max_depth = 10, min_samples_leaf = 5, random_state = 3)\n",
    "\n",
    "# fit decision tree\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = rf.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = rf.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# mean cv (runs on train set)\n",
    "mse_cv_scores = - cross_val_score(dt, x_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# compute the 10-folds CV rmse\n",
    "cv_rmse = (mse_cv_scores.mean())**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'the CV RMSE is {:.2f}'.format(cv_rmse))\n",
    "\n",
    "# find feature importance\n",
    "print(pd.Series(rf.feature_importances_, index = [\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]).sort_values(ascending = False))\n",
    "\n",
    "# predict\n",
    "prediction = rf.predict(x_holdout)\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 0.26; the CV RMSE is 0.25\n",
      "PetalWidthCm     0.5\n",
      "SepalLengthCm    0.3\n",
      "PetalLengthCm    0.2\n",
      "SepalWidthCm     0.0\n",
      "dtype: float64\n",
      "Prediction: 2.0 vs. Actual: 2.0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "y = iris['Species']\n",
    "y = y[:149]\n",
    "y_holdout = y[148:]\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "x = x[:149]\n",
    "x_holdout = x[148:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize decision tree\n",
    "rf = RandomForestClassifier(max_depth = 1, min_samples_leaf = 10, random_state = 3)\n",
    "\n",
    "# fit decision tree\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = rf.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = rf.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# mean cv (runs on train set)\n",
    "mse_cv_scores = - cross_val_score(dt, x_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# compute the 10-folds CV rmse\n",
    "cv_rmse = (mse_cv_scores.mean())**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'the CV RMSE is {:.2f}'.format(cv_rmse))\n",
    "\n",
    "# find feature importance\n",
    "print(pd.Series(rf.feature_importances_, index = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]).sort_values(ascending = False))\n",
    "\n",
    "# predict\n",
    "prediction = rf.predict(x_holdout)\n",
    "print(\"Prediction: {} vs. Actual: {}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Gradient Boosting\n",
    "- Application: Gradient Boosting is used for classification and regression problems\n",
    "- Description: Gradient Boosting is an ensemble learning method, building a multitude of decision trees sequentially with each model learning from the errors of its predecessors\n",
    "- Functionality:\n",
    "    * Boosting does not involve bootstrapping; instead each tree is fit on a modified version of the original data\n",
    "    * We start by fitting a simple model (i.e. decision tree)\n",
    "    * We then fit a new decision tree to the residuals of the model (we fit the tree using the residuals as the response)\n",
    "    * We then add this decision tree into the fitted function in order to update the residuals\n",
    "    * This way, we slowly improve the overall model in areas that it doesn't perform well yet\n",
    "- Limitations: Prone to overfitting, requires careful tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 8.14; the CV RMSE is 7.90\n",
      "CRIM    0.552882\n",
      "ZN      0.203198\n",
      "AGE     0.156695\n",
      "RAD     0.087225\n",
      "dtype: float64\n",
      "Prediction: 21.12 vs. Actual: 22.00\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "features = pd.DataFrame(boston[\"data\"], columns = boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "y = y[:505]\n",
    "y_holdout = y[504:]\n",
    "x = features[[\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]].values\n",
    "x = x[:505]\n",
    "x_holdout = x[504:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize decision tree\n",
    "gb = GradientBoostingRegressor(max_depth = 3, min_samples_leaf = 10, random_state = 3)\n",
    "\n",
    "# fit decision tree\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = gb.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = gb.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# mean cv (runs on train set)\n",
    "mse_cv_scores = - cross_val_score(dt, x_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# compute the 10-folds CV rmse\n",
    "cv_rmse = (mse_cv_scores.mean())**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'the CV RMSE is {:.2f}'.format(cv_rmse))\n",
    "\n",
    "# find feature importance\n",
    "print(pd.Series(gb.feature_importances_, index = [\"RAD\", \"ZN\", \"CRIM\", \"AGE\"]).sort_values(ascending = False))\n",
    "\n",
    "# predict\n",
    "prediction = gb.predict(x_holdout)\n",
    "print(\"Prediction: {:.2f} vs. Actual: {:.2f}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE is 0.18; the CV RMSE is 0.25\n",
      "PetalWidthCm     0.576158\n",
      "PetalLengthCm    0.415946\n",
      "SepalWidthCm     0.006801\n",
      "SepalLengthCm    0.001095\n",
      "dtype: float64\n",
      "Prediction: 2.0 vs. Actual: 2.0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "iris = pd.concat([pd.DataFrame(iris['data'], columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]), pd.DataFrame(iris['target'], columns = ['Species'])],\n",
    "                axis = 1)\n",
    "y = iris['Species']\n",
    "y = y[:149]\n",
    "y_holdout = y[148:]\n",
    "x = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "x = x[:149]\n",
    "x_holdout = x[148:]\n",
    "\n",
    "# split data into train and test \n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# initialize decision tree\n",
    "gb = GradientBoostingClassifier(max_depth = 2, min_samples_leaf = 10, random_state = 3)\n",
    "\n",
    "# fit decision tree\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on x_test\n",
    "y_pred_test = gb.predict(x_test)\n",
    "\n",
    "# make predictions on x_train\n",
    "y_pred_train = gb.predict(x_train)\n",
    "\n",
    "# test rmse\n",
    "test_rmse = MSE(y_test, y_pred_test)**(1/2)\n",
    "\n",
    "# mean cv (runs on train set)\n",
    "mse_cv_scores = - cross_val_score(dt, x_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# compute the 10-folds CV rmse\n",
    "cv_rmse = (mse_cv_scores.mean())**(1/2)\n",
    "\n",
    "# print results\n",
    "print('Test set RMSE is {:.2f};'.format(test_rmse), 'the CV RMSE is {:.2f}'.format(cv_rmse))\n",
    "\n",
    "# find feature importance\n",
    "print(pd.Series(gb.feature_importances_, index = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]).sort_values(ascending = False))\n",
    "\n",
    "# predict\n",
    "prediction = gb.predict(x_holdout)\n",
    "print(\"Prediction: {} vs. Actual: {}\".format(float(prediction), float(y_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Graph Networks\n",
    "- Application: Graph Networks are used to analyze and visualize several real world datasets such as social networks, web link data, molecular structures, geographical maps, images and text.\n",
    "- Description: Graph Networks analyze the relationships and dependencies among a set of items.\n",
    "- Functionality:\n",
    "    * A graph consists of nodes (items) and edges (relationships)\n",
    "    * Types: Undirected networks (edges have no direction, directed networks (edges have direction), weighted networks (edges have an assigned weight), multi-graphs (nodes with multiple edges), signed networks (edges with positive or negative sign) \n",
    "    * Path finding:\n",
    "        * Breath First Search algorithm: Finds the minimum number of edges (shortes path) between two nodes\n",
    "        * Depth First Search algorithm: Visits all of the edges in a network while avoiding circles\n",
    "    * Centrality:\n",
    "        * Degree centrality: (# of node neighbors) / (# of potential node neighbors)\n",
    "        * Betweenness centrality: # of times a node is present in the shortest path of two nodes: (# shortest path through a node) / (all possible shortest paths)\n",
    "        * Eigenvector centrality: if its connected to other important nodes (i.e. Google page rank)\n",
    "- Limitations: Entire graph needs to be processed simultaneously, which can be impractical for large graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: ['EWR', 'MEM', 'LGA', 'FLL', 'SEA', 'JFK', 'DEN', 'ORD', 'MIA', 'PBI', 'MCO', 'CMH', 'MSP', 'IAD', 'CLT', 'TPA', 'DCA', 'SJU', 'ATL', 'BHM', 'SRQ', 'MSY', 'DTW', 'LAX', 'JAX', 'RDU', 'MDW', 'DFW', 'IAH', 'SFO', 'STL', 'CVG', 'IND', 'RSW', 'BOS', 'CLE']\n",
      "Edges: [('EWR', 'MEM'), ('EWR', 'SEA'), ('EWR', 'MIA'), ('EWR', 'ORD'), ('EWR', 'MSP'), ('EWR', 'TPA'), ('EWR', 'MSY'), ('EWR', 'DFW'), ('EWR', 'IAH'), ('EWR', 'SFO'), ('EWR', 'CVG'), ('EWR', 'IND'), ('EWR', 'RDU'), ('EWR', 'IAD'), ('EWR', 'RSW'), ('EWR', 'BOS'), ('EWR', 'PBI'), ('EWR', 'LAX'), ('EWR', 'MCO'), ('EWR', 'SJU'), ('LGA', 'FLL'), ('LGA', 'ORD'), ('LGA', 'PBI'), ('LGA', 'CMH'), ('LGA', 'IAD'), ('LGA', 'CLT'), ('LGA', 'MIA'), ('LGA', 'DCA'), ('LGA', 'BHM'), ('LGA', 'RDU'), ('LGA', 'ATL'), ('LGA', 'TPA'), ('LGA', 'MDW'), ('LGA', 'DEN'), ('LGA', 'MSP'), ('LGA', 'DTW'), ('LGA', 'STL'), ('LGA', 'MCO'), ('LGA', 'CVG'), ('LGA', 'IAH'), ('FLL', 'JFK'), ('SEA', 'JFK'), ('JFK', 'DEN'), ('JFK', 'MCO'), ('JFK', 'TPA'), ('JFK', 'SJU'), ('JFK', 'ATL'), ('JFK', 'SRQ'), ('JFK', 'DCA'), ('JFK', 'DTW'), ('JFK', 'LAX'), ('JFK', 'JAX'), ('JFK', 'CLT'), ('JFK', 'PBI'), ('JFK', 'CLE'), ('JFK', 'IAD'), ('JFK', 'BOS')]\n",
      "Average Degree Centrality: 0.09047619047619045\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "airlines = pd.read_csv('datasets/Airlines.csv', index_col = 0)\n",
    "\n",
    "# initialize graph network\n",
    "FG = nx.from_pandas_edgelist(airlines, source = 'origin', target = 'dest', edge_attr = True)\n",
    "\n",
    "# draw the network\n",
    "nx.draw_networkx(FG)\n",
    "plt.show()\n",
    "\n",
    "# print nodes and edges\n",
    "print('Nodes:', FG.nodes())\n",
    "print('Edges:', FG.edges())\n",
    "\n",
    "# calculate average degree centrality\n",
    "print('Average Degree Centrality:', np.mean(np.asarray(list(nx.algorithms.degree_centrality(FG).values()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
