{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Log Loss Function\n",
    "- Description: The Log Loss Function is used to measure the performance of a classification model where the prediction is a probability between 0 and 1.\n",
    "- Functionality:\n",
    "    * We penalize wrong predictions by adding to the loss function and do not increase the loss function if a correct prediction was made by the classification model.\n",
    "    * Formula: $logloss = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{i=1}^{M} y_{ij} ln(p_{ij})$ where $N$ is the number of objects to classify, $M$ is the number of classes, $y_{ij}$ is 1 if the object $i$ belongs the the class $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that object $i$ belongs to class $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Accuracy, Precision, Recall, F1, R Squared Scores\n",
    "- Confusion Matrix:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1594/1*CPnO_bcdbE8FXTejQiV2dg.png\"\n",
    "     width=\"500\" height=\"300\" />\n",
    "     \n",
    "- Accuracy:\n",
    "    * Fraction of predictions our model got right\n",
    "    * Good measure to report to senior leadership as its an intuitive way of measuring performance\n",
    "    * $Accuracy = \\frac{n~correct~predictions}{n~total~preductions}$\n",
    "- Precision:\n",
    "    * How many of the actual positive cases did we find\n",
    "    * Good measure to determine model accuracy, when the cost of False Negative is high (i.e. fraud detection)\n",
    "    * $Recall = \\frac{True~Positive}{True~Positive + False~Negative} = \\frac{True~Positive}{Total~Actual~Positive}$\n",
    "- Recall:\n",
    "    * Of all our predicted positive cases, how many of them are actual positive cases\n",
    "    * Good measure to determine model accuracy, when the costs of False Positive is high (i.e. email spam detection)\n",
    "    * $Precision = \\frac{True~Positive}{True~Positive + False~Positive} = \\frac{True~Positive}{Total~Predicted~Positive}$\n",
    "- F1: \n",
    "    * Weighed average of precision and recall\n",
    "    * Good measure when you want to seek a balance between Precision and Recall\n",
    "    * $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "- R Squared:\n",
    "    * Provides a measure of how well observed outcomes are replicated by a model\n",
    "    * Measures the proportion of the variance in the target variable that is predictable from the model's features\n",
    "    * Adjusted R Squared takes into consideration the number of predictors, accounting for overfitting\n",
    "    * $ R^2 = 1 - \\frac{SSE}{SSE + SSR} = 1 - \\frac{SSR}{SST}$\n",
    "        * SSE (sum of squares of residuals): Measures the unexplained variance of the model, hence the standard deviation of the error $\\sum{(y_i - \\hat{y})}$\n",
    "        * SSR (sum of squares of regression): Meaures the explained variance of the model $\\sum{(\\hat{y_i} - \\bar{y})}$\n",
    "        * SST (total sum of squares): Measures the total variance of the data $\\sum{(y_i - \\bar{y})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Root Mean Square Error\n",
    "- Description: The Root Mean Square Error (RMSE) is used to measure the performance of regression models\n",
    "- Functionality: \n",
    "    * Measured the differences between predicted values by a model and the actual values\n",
    "    * The RMSE will have the the same unit as the dependent variable, meaning that there is no absolute good or bad value    \n",
    "    * The formulat to find the RMSE is: $RMSE = \\sqrt{\\sum_{i=1}^{N} \\frac{(\\hat{y_i} - y_i)^2}{N}}$\n",
    "    * There are different variations of this formula (i.e. Mean Squared Error)\n",
    "    * MSE consists of the sum of: the variance of the function, its squared bias and the irreducible error ($\\sigma^2$)\n",
    "- Application:\n",
    "    * Training set: Use training dataset to train model, and apply model to test dataset to predict; then measure error between prediction and actual\n",
    "    * Test set: Use train dataset to train model and apply the model back to the train data for predictions; then measure error between prediction and actual\n",
    "    * Cross-Validation: See below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Cross Validation\n",
    "- Description: Cross Validation is a resampling procedure used 1) to determine how well a model can be expected to perform on new data or 2) to find the minimum point of the training error to select the best hyperparameters or models\n",
    "- Functionality:\n",
    "    * We first split all our data into a training and test data set\n",
    "    * Random: Randomly divide training data into training and validation data\n",
    "    * K-fold:\n",
    "        * We then divide our training data into k-folds, with k being some arbitrary number\n",
    "        * Assume we have 5 folds, so we will train the model on 4 folds and evaluate its performance on the the last fold\n",
    "        * We repeat this process 4 more times (for the remaining folds), iterating over the 4 training and 1 test folds\n",
    "        * Once the 5 models were fit, we combine the validation results by i.e. averaging the results of the 5 rounds to give an estimate of the model's predictive performance\n",
    "        * Consider variance-bias trade-off when selecting k: the higher k the higher the variance and the lower the bias because the fitted models are very similar with higher k because the underlying data is almost the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Overfitting vs. Underfitting \n",
    "- Variance refers to the amount by which the model would change if we introduced it to new training data (how closely does our model model the data points) vs. Bias refers to the error by approximating a real-world problem with a much simpler model\n",
    "- In general, the more flexible/complicated a model is, the higher its variance and the lower its bias\n",
    "- Overfitting: \n",
    "    * Modeling error occuring when a model is too closely fit to the training set data points and hence captures its noise due to its high complexity\n",
    "    * The model suffers from high variance, meaning that the model will be inconsistent over different training/testing sets\n",
    "    * Identification: $training~error < test~error$ OR $CV~error > training~error$\n",
    "    * Counter-measure: Decrease the model complexity, increase the minimum samples per leaf\n",
    "- Underfitting:\n",
    "    * Modeling error occuring when a model cannot adequately capture the training data and hence does not fit the data well enough\n",
    "    * The model suffers from high bias, meaning that the model will not be predicting with high accuracy\n",
    "    * Identification: high $training~error$ in general OR $CV~error = training~error > test~error$\n",
    "    * Counter-measure: Increase the model complexity, decrease the minimum samples per leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Misc Notes\n",
    "- Problem of **high-dimensionality data** ($p >> n$): We can perfectly fit our data without any error\n",
    "- **Training MSE** usually underestimates the test MSE, because we fit the model to the training data, minimizing the error; models with many parameter will explain more variance in the training data, but might be unstable on the test data, hence we measure their performance including the price paid for additional predictors in order to eliminate noisy predictors (i.e. AIC, adjusted $R^2$, BIC)\n",
    "- **Bayes decision boundary**: Classifies an observation to the class for which $p(x)$ is largest; for 2 points the boundary is where the probability is equal to 50%; in general Bayes classifier produces the lowest possible error rate\n",
    "- **Regularization** (technique to deal with overfitting):\n",
    "    * **Subset Selection**:\n",
    "        * Goal: Reduce the number of features in the model only selecting the most predictive ones\n",
    "        * Fit $p^2$ separate least squared linear regression models for each possible combination of the $p$ predictors and then select the best model\n",
    "        * Selection is based on i.e. CV error, test set error or adjusted $R^2$ to account for the increase in predictors\n",
    "        * This approach is computationally intensive esp. for large $p$\n",
    "        * Alternatives are:\n",
    "            * **Forward Selection**: Claculate null model, then calculate all models that contain one additional predictor, choose models with smallest RSS, select best model using cross validation/AIC/BIC\n",
    "            * **Backward Selection**: Claculate full model, then calculate all models that contain all but one predictor, choose models with smallest RSS, select best model using cross validation/AIC/BIC\n",
    "            * The problem with this is that we commit to a fixed path which might not result in the selection of the optimal features\n",
    "    * **Shrinkage**:\n",
    "        * See chapters on lasso and ridge regression models (applied to fit a linear regression given a large number of predictors with questionable effect on the model's accuracy)\n",
    "    * **Dimension Reduction**:\n",
    "        * Above approach is focused on selecting the right predictors, this approach is focused on transforming the predictors; for more details see section on dimension reduction\n",
    "- Methods to make linear models non-linear:\n",
    "    * **Polynomial regression**: Rasining each of the predictor to a power; i.e. cubic regression uses three variables $X, X^2, X^3$\n",
    "    * **Step functions**: Cut range of a predictor into $K$ distinct regions, producing a qualitative variable to then fit a piecewise constant function\n",
    "    * **Regression splines**: Divide range of predictor into $K$ distinct regions, fit polynomial function within each region, functions are constrained so that they join smoothly at the region boundaries/knots\n",
    "    * **Smoothing splines**: Result from minimizing least squares subject only to a smoothness penalty (second derivative of function/roughness)\n",
    "    * **Local regression**: Divide data into regions based on their neighbors in which unique regressions are built and merged in a smooth way\n",
    "    * **Generalized additive models**: Add different models of multiple predictors; be aware of missing interaction if not specified separately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
